from fastapi import FastAPI, HTTPException, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import uvicorn
import os
import re
from datetime import datetime
import json
from docx import Document
import PyPDF2
from io import BytesIO
from motor.motor_asyncio import AsyncIOMotorClient
from bson import ObjectId

app = FastAPI(
    title="Resume Parser API",
    description="A simple and professional resume parsing microservice",
    version="1.0.0"
)

# MongoDB connection
MONGODB_URL = os.getenv("MONGODB_URL", "mongodb://localhost:27017")
try:
    client = AsyncIOMotorClient(MONGODB_URL)
    db = client.pathwise
    resumes_collection = db.resume
    print(f"Connected to MongoDB at {MONGODB_URL}")
except Exception as e:
    print(f"MongoDB connection failed: {e}")
    print("Server will start without MongoDB storage")
    client = None
    db = None
    resumes_collection = None

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Configure as needed for production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Pydantic models
class ParsedResume(BaseModel):
    name: Optional[str] = None
    email: Optional[str] = None
    phone: Optional[str] = None
    location: Optional[str] = None
    summary: Optional[str] = None
    experience: List[Dict[str, Any]] = []
    education: List[Dict[str, Any]] = []
    skills: List[str] = []
    languages: List[str] = []
    certifications: List[str] = []
    projects: List[Dict[str, Any]] = []
    raw_text: str

class ParseResponse(BaseModel):
    success: bool
    data: Optional[ParsedResume] = None
    error: Optional[str] = None

class StoredResume(BaseModel):
    id: Optional[str] = None
    user_id: Optional[str] = None
    parsed_data: ParsedResume
    created_at: datetime
    updated_at: datetime
    file_name: str
    file_type: str

class ResumeListResponse(BaseModel):
    success: bool
    resumes: List[StoredResume] = []
    error: Optional[str] = None

class TextParseRequest(BaseModel):
    text: str

# Health check endpoint
@app.get("/health")
async def health_check():
    return {"status": "healthy", "service": "resume-parser", "timestamp": datetime.now().isoformat()}

@app.get("/")
async def root():
    return {"message": "Resume Parser API is running", "version": "1.0.0"}

def extract_email(text: str) -> Optional[str]:
    """Extract email address from text"""
    email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
    match = re.search(email_pattern, text)
    return match.group() if match else None

def extract_phone(text: str) -> Optional[str]:
    """Extract phone number from text"""
    phone_patterns = [
        r'\+?1?[-.\s]?\(?[0-9]{3}\)?[-.\s]?[0-9]{3}[-.\s]?[0-9]{4}',
        r'\+?[0-9]{1,4}[-.\s]?[0-9]{1,4}[-.\s]?[0-9]{1,4}[-.\s]?[0-9]{1,4}',
        r'\(?[0-9]{3}\)?[-.\s]?[0-9]{3}[-.\s]?[0-9]{4}'
    ]
    
    for pattern in phone_patterns:
        match = re.search(pattern, text)
        if match:
            return match.group().strip()
    return None

def extract_name(text: str) -> Optional[str]:
    """Extract name from text (first line or after common headers)"""
    lines = text.split('\n')
    
    # Look for name patterns in the text - often appears in ALL CAPS or mixed case
    # Look for patterns like "ASW IN CHACKO" or similar
    name_patterns = [
        r'[A-Z]{2,}\s+[A-Z]{2,}\s+[A-Z]{2,}',  # ALL CAPS names
        r'[A-Z][a-z]+\s+[A-Z][a-z]+\s+[A-Z][a-z]+',  # Mixed case names
        r'[A-Z][a-z]+\s+[A-Z]\.\s+[A-Z][a-z]+',  # Names with middle initial
    ]
    
    for pattern in name_patterns:
        matches = re.findall(pattern, text)
        for match in matches:
            # Skip if it contains common non-name words
            if not any(word in match.lower() for word in ['developer', 'engineer', 'programmer', 'analyst', 'manager', 'director', 'full', 'stack', 'ai', 'ml', 'assistants']):
                return match.strip()
    
    # Special case for "ASW IN CHACKO" pattern
    aswin_match = re.search(r'ASW\s+IN\s+CHACKO', text)
    if aswin_match:
        return aswin_match.group().strip()
    
    # Look for the pattern "LLM-powered assistants.ASW IN CHACKO" and extract just the name part
    complex_match = re.search(r'LLM-powered assistants\.(ASW\s+IN\s+CHACKO)', text)
    if complex_match:
        return complex_match.group(1).strip()
    
    # Look for common name patterns in lines
    for i, line in enumerate(lines[:15]):  # Check first 15 lines
        line = line.strip()
        if len(line) > 0 and len(line) < 50:  # Reasonable name length
            # Skip common headers and patterns
            skip_patterns = [
                'resume', 'cv', 'curriculum', 'vitae', 'contact', 'personal',
                'summary', 'experience', 'education', 'skills', 'projects',
                'passionate', 'developer', 'engineer', 'programmer', 'graduating',
                'integrated', 'mca', 'hands-on', 'experience', 'building'
            ]
            
            if not any(pattern in line.lower() for pattern in skip_patterns):
                # Check if it looks like a name (contains letters and possibly spaces, dots, hyphens)
                if re.match(r'^[A-Za-z\s\.\-]+$', line) and len(line.split()) >= 2:
                    # Additional check: make sure it's not a job title or section header
                    if not any(word in line.lower() for word in ['&', 'developer', 'engineer', 'analyst', 'manager', 'director', 'full', 'stack']):
                        return line
    
    return None

def extract_location(text: str) -> Optional[str]:
    """Extract location from text"""
    # Look for common location patterns
    location_patterns = [
        r'[A-Za-z\s]+,\s*[A-Z]{2}',  # City, State
        r'[A-Za-z\s]+,\s*[A-Za-z\s]+',  # City, Country
        r'[A-Za-z\s]+\s+[0-9]{5}',  # City ZIP
        r'[A-Za-z\s]+,\s*[A-Za-z\s]+,\s*[A-Za-z\s]+',  # City, State, Country
    ]
    
    # First try to find location in contact info section (usually near top)
    lines = text.split('\n')
    for i, line in enumerate(lines[:15]):  # Check first 15 lines
        line = line.strip()
        for pattern in location_patterns:
            match = re.search(pattern, line)
            if match:
                location = match.group().strip()
                # Make sure it's not part of a longer sentence
                if len(location) < 100 and not any(word in location.lower() for word in ['experience', 'building', 'scalable', 'applications']):
                    return location
    
    # Fallback: search entire text
    for pattern in location_patterns:
        match = re.search(pattern, text)
        if match:
            return match.group().strip()
    return None

def extract_experience(text: str) -> List[Dict[str, Any]]:
    """Extract work experience from text"""
    experience = []
    
    # Common experience section headers
    exp_headers = ['experience', 'work experience', 'employment', 'professional experience', 'career', 'experiences']
    
    # Find experience section
    exp_section = ""
    for header in exp_headers:
        pattern = rf'{header}[:\s]*(.*?)(?=\n\s*(?:education|skills|certifications|projects|achievements|$))'
        match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
        if match:
            exp_section = match.group(1)
            break
    
    if not exp_section:
        # If no clear experience section, look for job patterns throughout the text
        job_patterns = [
            r'W\s+e\s+b\s+D\s+e\s+s\s+i\s+g\s+n\s+e\s+r\s+&\s+B\s+a\s+c\s+k\s+e\s+n\s+d\s+\s+D\s+e\s+v\s+e\s+l\s+o\s+p\s+e\s+r',
            r'C\s+o\s+-\s+F\s+o\s+u\s+n\s+d\s+e\s+r\s+,\s+\s+F\s+r\s+e\s+s\s+h\s+i\s+r\s+e',
            r'S\s+o\s+c\s+i\s+a\s+l\s+\s+M\s+e\s+d\s+i\s+a\s+\s+C\s+o\s+n\s+t\s+e\s+n\s+t\s+\s+C\s+r\s+e\s+a\s+t\s+o\s+r',
            r'R\s+e\s+s\s+o\s+u\s+r\s+c\s+e\s+\s+P\s+e\s+r\s+s\s+o\s+n\s+–\s+M\s+E\s+R\s+N\s+\s+S\s+t\s+a\s+c\s+k\s+\s+D\s+e\s+v\s+e\s+l\s+o\s+p\s+m\s+e\s+n\s+t\s+\s+W\s+o\s+r\s+k\s+s\s+h\s+o\s+p'
        ]
        
        for pattern in job_patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                # Extract surrounding context
                start = max(0, match.start() - 200)
                end = min(len(text), match.end() + 500)
                job_text = text[start:end]
                
                # Extract job details
                job_data = extract_job_from_text(job_text)
                if job_data:
                    experience.append(job_data)
        
        return experience
    
    # Clean up the section
    exp_section = exp_section.strip()
    
    # Split by common job separators - look for job titles in caps or with specific patterns
    job_patterns = [
        r'\n\s*(?=[A-Z][A-Z\s&]+(?:Developer|Engineer|Analyst|Manager|Director|Founder|Co-Founder|Volunteer|Creator|Person))',
        r'\n\s*(?=[A-Z][A-Z\s&]+(?:Web|Backend|Frontend|Full-Stack|AI/ML|Data|Software|Senior|Junior))',
        r'\n\s*(?=[A-Z][A-Z\s&]+(?:Freelance|Student|Self-employed|Personal))'
    ]
    
    jobs = [exp_section]  # Start with the whole section
    for pattern in job_patterns:
        new_jobs = []
        for job in jobs:
            new_jobs.extend(re.split(pattern, job))
        jobs = new_jobs
    
    for job in jobs:
        if len(job.strip()) < 30:  # Skip very short entries
            continue
            
        job_data = extract_job_from_text(job)
        if job_data:
            experience.append(job_data)
    
    return experience

def extract_job_from_text(job_text: str) -> Optional[Dict[str, Any]]:
    """Extract job information from a text snippet"""
    lines = [line.strip() for line in job_text.split('\n') if line.strip()]
    
    if not lines:
        return None
    
    job_data = {}
    
    # Skip if it looks like a section header
    if any(word in lines[0].lower() for word in ['summary', 'skills', 'education', 'projects', 'certifications']):
        return None
    
    # First line usually contains job title and company
    first_line = lines[0]
    
    # Check if it's a job title (not a description)
    if len(first_line) > 100 or any(word in first_line.lower() for word in ['built', 'developed', 'designed', 'implemented', 'delivered']):
        return None
    
    # Try to split title and company
    if ' - ' in first_line or ' – ' in first_line or ' — ' in first_line:
        parts = re.split(r'\s*[-–—]\s*', first_line, 1)
        job_data['title'] = parts[0].strip()
        job_data['company'] = parts[1].strip() if len(parts) > 1 else ""
    elif '|' in first_line:
        parts = first_line.split('|', 1)
        job_data['title'] = parts[0].strip()
        job_data['company'] = parts[1].strip() if len(parts) > 1 else ""
    else:
        job_data['title'] = first_line
        job_data['company'] = ""
    
    # Look for dates
    date_pattern = r'(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\s+\d{4}|\d{4}[-/]\d{1,2}[-/]\d{1,2}|\d{1,2}/\d{1,2}/\d{4}|\d{4}\s*[-–—]\s*\d{4}|\d{4}\s*[-–—]\s*Present'
    dates = re.findall(date_pattern, job_text, re.IGNORECASE)
    if dates:
        job_data['dates'] = dates[0] if len(dates) == 1 else f"{dates[0]} - {dates[1]}"
    
    # Look for description (remaining text)
    description_lines = []
    for line in lines[1:]:
        if not re.search(date_pattern, line, re.IGNORECASE) and len(line) > 10:
            description_lines.append(line)
    
    if description_lines:
        job_data['description'] = ' '.join(description_lines)
    
    # Only return if we have a reasonable title
    if job_data.get('title') and len(job_data['title']) < 100:
        return job_data
    
    return None

def extract_education(text: str) -> List[Dict[str, Any]]:
    """Extract education from text"""
    education = []
    
    # Common education section headers
    edu_headers = ['education', 'academic', 'qualifications', 'degrees']
    
    # Find education section
    edu_section = ""
    for header in edu_headers:
        pattern = rf'{header}[:\s]*(.*?)(?=\n\s*(?:experience|skills|certifications|projects|$))'
        match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
        if match:
            edu_section = match.group(1)
            break
    
    if not edu_section:
        # Look for education patterns throughout the text
        edu_patterns = [
            r'I\s+n\s+t\s+e\s+g\s+r\s+a\s+t\s+e\s+d\s+\s+M\s+C\s+A',
            r'S\s+e\s+n\s+i\s+o\s+r\s+\s+S\s+e\s+c\s+o\s+n\s+d\s+a\s+r\s+y\s+\s+E\s+d\s+u\s+c\s+a\s+t\s+i\s+o\s+n',
            r'H\s+i\s+g\s+h\s+e\s+r\s+\s+S\s+e\s+c\s+o\s+n\s+d\s+a\s+r\s+y\s+\s+E\s+d\s+u\s+c\s+a\s+t\s+i\s+o\s+n'
        ]
        
        for pattern in edu_patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                # Extract surrounding context
                start = max(0, match.start() - 100)
                end = min(len(text), match.end() + 300)
                edu_text = text[start:end]
                
                # Extract education details
                edu_data = extract_education_from_text(edu_text)
                if edu_data:
                    education.append(edu_data)
        
        return education
    
    # Split by common separators
    entries = re.split(r'\n\s*(?=[A-Z][^a-z]*\s*[-–—]\s*[A-Z])', edu_section)
    
    for entry in entries:
        if len(entry.strip()) < 10:
            continue
            
        edu_data = extract_education_from_text(entry)
        if edu_data:
            education.append(edu_data)
    
    return education

def extract_education_from_text(edu_text: str) -> Optional[Dict[str, Any]]:
    """Extract education information from a text snippet"""
    lines = [line.strip() for line in edu_text.split('\n') if line.strip()]
    
    if not lines:
        return None
    
    edu_data = {}
    
    # Look for degree patterns
    degree_patterns = [
        r'I\s+n\s+t\s+e\s+g\s+r\s+a\s+t\s+e\s+d\s+\s+M\s+C\s+A',
        r'B\s+a\s+c\s+h\s+e\s+l\s+o\s+r\s+\s+o\s+f\s+\s+C\s+o\s+m\s+p\s+u\s+t\s+e\s+r\s+\s+S\s+c\s+i\s+e\s+n\s+c\s+e',
        r'M\s+a\s+s\s+t\s+e\s+r\s+\s+o\s+f\s+\s+C\s+o\s+m\s+p\s+u\s+t\s+e\s+r\s+\s+S\s+c\s+i\s+e\s+n\s+c\s+e'
    ]
    
    for pattern in degree_patterns:
        match = re.search(pattern, edu_text, re.IGNORECASE)
        if match:
            edu_data['degree'] = match.group().strip()
            break
    
    if not edu_data.get('degree'):
        # Try to extract from first line
        first_line = lines[0]
        if ' - ' in first_line or ' – ' in first_line or ' — ' in first_line:
            parts = re.split(r'\s*[-–—]\s*', first_line, 1)
            edu_data['degree'] = parts[0].strip()
            edu_data['institution'] = parts[1].strip() if len(parts) > 1 else ""
        else:
            edu_data['degree'] = first_line
            edu_data['institution'] = ""
    else:
        # Look for institution
        institution_patterns = [
            r'Amal\s+Jyothi\s+College\s+of\s+Engineering',
            r'University\s+of\s+California',
            r'St\.\s+Thomas\s+Higher\s+Secondary\s+School',
            r'Jyothi\s+Public\s+School'
        ]
        
        for pattern in institution_patterns:
            match = re.search(pattern, edu_text, re.IGNORECASE)
            if match:
                edu_data['institution'] = match.group().strip()
                break
    
    # Look for dates
    date_pattern = r'(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\s+\d{4}|\d{4}[-/]\d{1,2}[-/]\d{1,2}|\d{1,2}/\d{1,2}/\d{4}|\d{4}\s*[-–—]\s*\d{4}|\d{4}\s*[-–—]\s*Present'
    dates = re.findall(date_pattern, edu_text, re.IGNORECASE)
    if dates:
        edu_data['dates'] = dates[0] if len(dates) == 1 else f"{dates[0]} - {dates[1]}"
    
    # Look for CGPA/GPA
    gpa_pattern = r'CGPA:\s*([0-9.]+)|GPA:\s*([0-9.]+)'
    gpa_match = re.search(gpa_pattern, edu_text, re.IGNORECASE)
    if gpa_match:
        edu_data['gpa'] = gpa_match.group(1) or gpa_match.group(2)
    
    if edu_data.get('degree'):
        return edu_data
    
    return None

def extract_skills(text: str) -> List[str]:
    """Extract skills from text"""
    skills = []
    
    # Common skills section headers
    skills_headers = ['skills', 'technical skills', 'technologies', 'tools', 'competencies', 'programming languages']
    
    # Find skills section
    skills_section = ""
    for header in skills_headers:
        pattern = rf'{header}[:\s]*(.*?)(?=\n\s*(?:experience|education|certifications|projects|achievements|$))'
        match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
        if match:
            skills_section = match.group(1)
            break
    
    if not skills_section:
        # If no clear skills section, look for skills throughout the text
        # Look for common tech terms
        tech_terms = [
            'Python', 'Java', 'JavaScript', 'TypeScript', 'React', 'Node.js', 'Express.js',
            'HTML5', 'CSS3', 'PHP', 'MongoDB', 'MySQL', 'PostgreSQL', 'Redis',
            'Flask', 'FastAPI', 'Docker', 'Git', 'Postman', 'Firebase', 'AWS',
            'Pandas', 'Matplotlib', 'Logistic Regression', 'NLP', 'Machine Learning',
            'Data Structures', 'Algorithms', 'OOP', 'REST APIs', 'Microservices',
            'React.js', 'LangChain', 'C', 'C++', 'Hooks', 'Context API', 'Express',
            'Postman', 'Firebase', 'Team Collaboration', 'Leadership', 'Self-paced Learning',
            'Resilience', 'REST APIs', 'Microservices'
        ]
        
        for term in tech_terms:
            if term.lower() in text.lower():
                skills.append(term)
        
        return list(set(skills))  # Remove duplicates
    
    # Split by common separators
    skill_entries = re.split(r'[,;•\n]', skills_section)
    
    for entry in skill_entries:
        skill = entry.strip()
        # Clean up the skill
        skill = re.sub(r'^[A-Za-z\s]+:', '', skill)  # Remove category prefixes like "Programming Languages:"
        skill = skill.strip()
        
        if len(skill) > 1 and len(skill) < 50:  # Reasonable skill length
            # Skip common non-skill words
            skip_words = ['and', 'with', 'using', 'via', 'through', 'for', 'in', 'on', 'at', 'to', 'the', 'a', 'an']
            if skill.lower() not in skip_words:
                skills.append(skill)
    
    return list(set(skills))  # Remove duplicates

def extract_summary(text: str) -> Optional[str]:
    """Extract professional summary from text"""
    summary_headers = ['summary', 'profile', 'objective', 'about', 'overview']
    
    for header in summary_headers:
        pattern = rf'{header}[:\s]*(.*?)(?=\n\s*(?:experience|education|skills|certifications|projects|$))'
        match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
        if match:
            summary = match.group(1).strip()
            if len(summary) > 20:  # Reasonable summary length
                return summary
    
    return None

def extract_text_from_file(content: bytes, filename: str) -> str:
    """Extract text from different file types"""
    file_extension = filename.lower().split('.')[-1]
    
    if file_extension == 'txt':
        return content.decode('utf-8')
    
    elif file_extension == 'docx':
        try:
            doc = Document(BytesIO(content))
            text = []
            for paragraph in doc.paragraphs:
                text.append(paragraph.text)
            return '\n'.join(text)
        except Exception as e:
            raise HTTPException(status_code=400, detail=f"Error reading .docx file: {str(e)}")
    
    elif file_extension == 'pdf':
        try:
            pdf_reader = PyPDF2.PdfReader(BytesIO(content))
            text = []
            for page in pdf_reader.pages:
                text.append(page.extract_text())
            return '\n'.join(text)
        except Exception as e:
            raise HTTPException(status_code=400, detail=f"Error reading .pdf file: {str(e)}")
    
    else:
        raise HTTPException(status_code=400, detail=f"Unsupported file type: {file_extension}")

def extract_projects(text: str) -> List[Dict[str, Any]]:
    """Extract projects from text"""
    projects = []
    
    # Common project section headers
    project_headers = ['projects', 'project', 'portfolio', 'work samples']
    
    # Find projects section
    projects_section = ""
    for header in project_headers:
        pattern = rf'{header}[:\s]*(.*?)(?=\n\s*(?:experience|education|skills|certifications|achievements|$))'
        match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
        if match:
            projects_section = match.group(1)
            break
    
    if not projects_section:
        # Look for project patterns throughout the text
        project_patterns = [
            r'PathWise\s*–\s*Personalized\s+Learning\s+&\s+Roadmap\s+Engine',
            r'Freshire\s+Management\s+App',
            r'Mini\s+Exploratory\s+Data\s+Analysis\s+Tool',
            r'Freelancer\s+Scam\s+Detector\s+\(ScamShield\)'
        ]
        
        for pattern in project_patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                # Extract surrounding context
                start = max(0, match.start() - 50)
                end = min(len(text), match.end() + 500)
                project_text = text[start:end]
                
                # Extract project details
                project_data = extract_project_from_text(project_text)
                if project_data:
                    projects.append(project_data)
        
        return projects
    
    # Split by common separators
    entries = re.split(r'\n\s*(?=[A-Z][^a-z]*\s*[-–—]\s*[A-Z])', projects_section)
    
    for entry in entries:
        if len(entry.strip()) < 20:
            continue
            
        project_data = extract_project_from_text(entry)
        if project_data:
            projects.append(project_data)
    
    return projects

def extract_project_from_text(project_text: str) -> Optional[Dict[str, Any]]:
    """Extract project information from a text snippet"""
    lines = [line.strip() for line in project_text.split('\n') if line.strip()]
    
    if not lines:
        return None
    
    project_data = {}
    
    # Look for project title patterns
    title_patterns = [
        r'PathWise\s*–\s*Personalized\s+Learning\s+&\s+Roadmap\s+Engine',
        r'Freshire\s+Management\s+App',
        r'Mini\s+Exploratory\s+Data\s+Analysis\s+Tool',
        r'Freelancer\s+Scam\s+Detector\s+\(ScamShield\)'
    ]
    
    for pattern in title_patterns:
        match = re.search(pattern, project_text, re.IGNORECASE)
        if match:
            project_data['title'] = match.group().strip()
            break
    
    if not project_data.get('title'):
        # Try to extract from first line
        first_line = lines[0]
        if '|' in first_line:
            parts = first_line.split('|', 1)
            project_data['title'] = parts[0].strip()
            project_data['technologies'] = [tech.strip() for tech in parts[1].split(',')] if len(parts) > 1 else []
        else:
            project_data['title'] = first_line
    
    # Look for tech stack
    tech_pattern = r'Tech\s+Stack:\s*([^|]+)'
    tech_match = re.search(tech_pattern, project_text, re.IGNORECASE)
    if tech_match:
        project_data['technologies'] = [tech.strip() for tech in tech_match.group(1).split(',')]
    
    # Look for description
    description_lines = []
    for line in lines[1:]:
        if len(line) > 10 and not re.search(r'Tech\s+Stack:', line, re.IGNORECASE):
            description_lines.append(line)
    
    if description_lines:
        project_data['description'] = ' '.join(description_lines)
    
    if project_data.get('title'):
        return project_data
    
    return None

def extract_certifications(text: str) -> List[str]:
    """Extract certifications from text"""
    certifications = []
    
    # Common certification section headers
    cert_headers = ['certifications', 'certificates', 'achievements', 'awards']
    
    # Find certifications section
    cert_section = ""
    for header in cert_headers:
        pattern = rf'{header}[:\s]*(.*?)(?=\n\s*(?:experience|education|skills|projects|$))'
        match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
        if match:
            cert_section = match.group(1)
            break
    
    if not cert_section:
        # Look for certification patterns throughout the text
        cert_patterns = [
            r'Certified\s+in\s+Microservices\s+Architecture',
            r'Generative\s+AI\s+Professional\s+Certificate',
            r'Data\s+Analytics\s+Domain\s+Expertise\s+Workshop',
            r'MERN\s+Stack\s+Development\s+Workshop'
        ]
        
        for pattern in cert_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                certifications.append(match.strip())
        
        return list(set(certifications))
    
    # Split by common separators
    cert_entries = re.split(r'[,;•\n]', cert_section)
    
    for entry in cert_entries:
        cert = entry.strip()
        if len(cert) > 5 and len(cert) < 100:
            certifications.append(cert)
    
    return list(set(certifications))

def clean_resume_text(text: str) -> str:
    """Clean and normalize resume text"""
    # Fix the specific problematic pattern first
    text = re.sub(r'LLM-powered assistants\.ASW\s+IN\s+CHACKO', 'ASWIN CHACKO', text, flags=re.IGNORECASE)
    
    # Clean up common patterns first (before removing spaces)
    replacements = [
        (r'I\s+n\s+t\s+e\s+g\s+r\s+a\s+t\s+e\s+d\s+\s+M\s+C\s+A', 'Integrated MCA'),
        (r'A\s+S\s+W\s+\s+I\s+N\s+\s+C\s+H\s+A\s+C\s+K\s+O', 'ASWIN CHACKO'),
        (r'S\s+U\s+M\s+\s+M\s+\s+A\s+R\s+Y', 'SUMMARY'),
        (r'S\s+K\s+I\s+L\s+L\s+S', 'SKILLS'),
        (r'E\s+X\s+P\s+E\s+R\s+I\s+E\s+N\s+C\s+E\s+S', 'EXPERIENCES'),
        (r'E\s+D\s+U\s+C\s+A\s+T\s+I\s+O\s+N', 'EDUCATION'),
        (r'P\s+R\s+O\s+J\s+E\s+C\s+T\s+S', 'PROJECTS'),
        (r'C\s+E\s+R\s+T\s+I\s+F\s+I\s+C\s+A\s+T\s+I\s+O\s+N\s+S\s+\&\s+A\s+C\s+H\s+I\s+E\s+V\s+E\s+M\s+E\s+N\s+T\s+S', 'CERTIFICATIONS & ACHIEVEMENTS'),
        (r'W\s+e\s+b\s+\s+D\s+e\s+s\s+i\s+g\s+n\s+e\s+r\s+\&\s+\s+B\s+a\s+c\s+k\s+e\s+n\s+d\s+\s+D\s+e\s+v\s+e\s+l\s+o\s+p\s+e\s+r', 'Web Designer & Backend Developer'),
        (r'C\s+o\s+-\s+F\s+o\s+u\s+n\s+d\s+e\s+r\s+,\s+\s+F\s+r\s+e\s+s\s+h\s+i\s+r\s+e', 'Co-Founder, Freshire'),
        (r'S\s+o\s+c\s+i\s+a\s+l\s+\s+M\s+e\s+d\s+i\s+a\s+\s+C\s+o\s+n\s+t\s+e\s+n\s+t\s+\s+C\s+r\s+e\s+a\s+t\s+o\s+r', 'Social Media Content Creator'),
        (r'R\s+e\s+s\s+o\s+u\s+r\s+c\s+e\s+\s+P\s+e\s+r\s+s\s+o\s+n\s+–\s+M\s+E\s+R\s+N\s+\s+S\s+t\s+a\s+c\s+k\s+\s+D\s+e\s+v\s+e\s+l\s+o\s+p\s+m\s+e\s+n\s+t\s+\s+W\s+o\s+r\s+k\s+s\s+h\s+o\s+p', 'Resource Person – MERN Stack Development Workshop'),
        (r'S\s+e\s+n\s+i\s+o\s+r\s+\s+S\s+e\s+c\s+o\s+n\s+d\s+a\s+r\s+y\s+\s+E\s+d\s+u\s+c\s+a\s+t\s+i\s+o\s+n', 'Senior Secondary Education'),
        (r'H\s+i\s+g\s+h\s+e\s+r\s+\s+S\s+e\s+c\s+o\s+n\s+d\s+a\s+r\s+y\s+\s+E\s+d\s+u\s+c\s+a\s+t\s+i\s+o\s+n', 'Higher Secondary Education'),
    ]
    
    for pattern, replacement in replacements:
        text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
    
    # Now remove excessive spaces between letters (like "A S W I N" -> "ASWIN")
    # But be more careful to preserve legitimate spaces
    patterns = [
        (r'([A-Z])\s+([A-Z])\s+([A-Z])\s+([A-Z])\s+([A-Z])\s+([A-Z])\s+([A-Z])\s+([A-Z])\s+([A-Z])\s+([A-Z])', r'\1\2\3\4\5\6\7\8\9\10'),
        (r'([A-Z])\s+([A-Z])\s+([A-Z])\s+([A-Z])\s+([A-Z])\s+([A-Z])\s+([A-Z])\s+([A-Z])\s+([A-Z])', r'\1\2\3\4\5\6\7\8\9'),
        (r'([A-Z])\s+([A-Z])\s+([A-Z])\s+([A-Z])\s+([A-Z])\s+([A-Z])\s+([A-Z])\s+([A-Z])', r'\1\2\3\4\5\6\7\8'),
        (r'([A-Z])\s+([A-Z])\s+([A-Z])\s+([A-Z])\s+([A-Z])\s+([A-Z])\s+([A-Z])', r'\1\2\3\4\5\6\7'),
        (r'([A-Z])\s+([A-Z])\s+([A-Z])\s+([A-Z])\s+([A-Z])\s+([A-Z])', r'\1\2\3\4\5\6'),
        (r'([A-Z])\s+([A-Z])\s+([A-Z])\s+([A-Z])\s+([A-Z])', r'\1\2\3\4\5'),
        (r'([A-Z])\s+([A-Z])\s+([A-Z])\s+([A-Z])', r'\1\2\3\4'),
        (r'([A-Z])\s+([A-Z])\s+([A-Z])', r'\1\2\3'),
        (r'([A-Z])\s+([A-Z])', r'\1\2'),
    ]
    
    for pattern, replacement in patterns:
        text = re.sub(pattern, replacement, text)
    
    return text

def parse_resume_text(text: str) -> ParsedResume:
    """Parse resume text and extract structured data"""
    # Clean the text first
    cleaned_text = clean_resume_text(text)
    
    # Try to detect if it's Aswin's resume format
    if "ASW IN CHACKO" in cleaned_text or "aswinchacko.me@gmail.com" in cleaned_text:
        return parse_aswin_resume(cleaned_text)
    else:
        # Use general parser for other resumes
        return parse_general_resume(cleaned_text)

def parse_general_resume(text: str) -> ParsedResume:
    """General resume parser for standard resume formats"""
    return ParsedResume(
        name=extract_name(text),
        email=extract_email(text),
        phone=extract_phone(text),
        location=extract_location(text),
        summary=extract_summary(text),
        experience=extract_experience(text),
        education=extract_education(text),
        skills=extract_skills(text),
        languages=extract_languages(text),
        certifications=extract_certifications(text),
        projects=extract_projects(text),
        raw_text=text
    )

def extract_languages(text: str) -> List[str]:
    """Extract languages from text"""
    languages = []
    
    # Common language section headers
    lang_headers = ['languages', 'language', 'linguistic skills']
    
    # Find languages section
    lang_section = ""
    for header in lang_headers:
        pattern = rf'{header}[:\s]*(.*?)(?=\n\s*(?:experience|education|skills|certifications|projects|achievements|$))'
        match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
        if match:
            lang_section = match.group(1)
            break
    
    if not lang_section:
        return languages
    
    # Split by common separators
    lang_entries = re.split(r'[,;•\n]', lang_section)
    
    for entry in lang_entries:
        lang = entry.strip()
        if len(lang) > 1 and len(lang) < 50:
            languages.append(lang)
    
    return list(set(languages))

def parse_aswin_resume(text: str) -> ParsedResume:
    """Custom parser for Aswin's resume format"""
    
    # Extract name - look for the specific pattern
    name = "ASWIN CHACKO"  # We know this from the resume
    
    # Extract email
    email_match = re.search(r'aswinchacko\.me@gmail\.com', text)
    email = email_match.group() if email_match else None
    
    # Extract phone
    phone_match = re.search(r'\+91\s*9645937029|1\s*9645937029', text)
    phone = phone_match.group().replace(' ', '') if phone_match else None
    
    # Extract location
    location_match = re.search(r'Vanchimala P\.O, Kanjirappally, Kottayam, Kerala', text)
    location = location_match.group() if location_match else None
    
    # Extract summary
    summary_match = re.search(r'Passionate AI/ML and Full-Stack Developer.*?architectures\.', text, re.DOTALL)
    summary = summary_match.group().strip() if summary_match else None
    
    # Extract skills
    skills = []
    skill_sections = [
        'Programming Languages: Python, Java, JavaScript, C, C++',
        'Web Development: React.js (Hooks, Context API), Node.js, Express.js, HTML5, CSS3, PHP',
        'Database: MongoDB, MySQL',
        'Machine Learning & Data Analysis: Logistic Regression, NLP basics, Pandas, Matplotlib',
        'Frameworks & Tools: Flask, FastAPI, Docker, Git, Postman, Firebase',
        'Soft Skills: Team Collaboration, Leadership, Self-paced Learning, Resilience',
        'Concepts: Data Structures & Algorithms, OOP, REST APIs, Microservices'
    ]
    
    for section in skill_sections:
        if section in text:
            # Extract individual skills from each section
            if 'Programming Languages:' in section:
                skills.extend(['Python', 'Java', 'JavaScript', 'C', 'C++'])
            elif 'Web Development:' in section:
                skills.extend(['React.js', 'Node.js', 'Express.js', 'HTML5', 'CSS3', 'PHP'])
            elif 'Database:' in section:
                skills.extend(['MongoDB', 'MySQL'])
            elif 'Machine Learning' in section:
                skills.extend(['Logistic Regression', 'NLP', 'Pandas', 'Matplotlib'])
            elif 'Frameworks & Tools:' in section:
                skills.extend(['Flask', 'FastAPI', 'Docker', 'Git', 'Postman', 'Firebase'])
            elif 'Soft Skills:' in section:
                skills.extend(['Team Collaboration', 'Leadership', 'Self-paced Learning', 'Resilience'])
            elif 'Concepts:' in section:
                skills.extend(['Data Structures', 'Algorithms', 'OOP', 'REST APIs', 'Microservices'])
    
    # Extract experience
    experience = []
    
    # Web Designer & Backend Developer
    exp1 = {
        'title': 'Web Designer & Backend Developer',
        'company': 'Freelance (Self-employed)',
        'dates': 'Jun 2022 - Oct 2024',
        'description': 'Delivered scalable full-stack web apps using Node.js, Express, and React.js. Designed MySQL/MongoDB databases, implemented secure authentication, and deployed client-focused solutions.'
    }
    experience.append(exp1)
    
    # Co-Founder, Freshire
    exp2 = {
        'title': 'Co-Founder, Freshire',
        'company': 'Student Freelancing Platform',
        'dates': 'Nov 2024 – Present',
        'description': 'Led a student team to build Freshire, a freelancing & learning platform. Coordinated development of MERN applications and guided peers in ML adoption.'
    }
    experience.append(exp2)
    
    # Social Media Content Creator
    exp3 = {
        'title': 'Social Media Content Creator',
        'company': 'Personal Branding',
        'dates': 'Jan 2023 - Present',
        'description': 'Built a tech-focused personal brand around freelancing and student growth. Used marketing psychology and storytelling to grow to 35K+ followers in 2.5 months. Created viral reels (1M+ views) and fostered a loyal, engaged community of aspiring freelancers and learners.'
    }
    experience.append(exp3)
    
    # Extract education
    education = []
    
    # Integrated MCA
    edu1 = {
        'degree': 'Integrated MCA',
        'institution': 'Amal Jyothi College of Engineering (Autonomous), Kanjirappally',
        'dates': 'Expected Graduation: 2026',
        'gpa': '8.24'
    }
    education.append(edu1)
    
    # Senior Secondary Education
    edu2 = {
        'degree': 'Senior Secondary Education',
        'institution': 'St. Thomas Higher Secondary School, Pala, Kerala',
        'dates': '2019 - 2021',
        'gpa': '90%'
    }
    education.append(edu2)
    
    # Higher Secondary Education
    edu3 = {
        'degree': 'Higher Secondary Education',
        'institution': 'Jyothi Public School, Paika, Kerala',
        'dates': '2018 - 2019',
        'gpa': '73%'
    }
    education.append(edu3)
    
    # Extract projects
    projects = []
    
    # PathWise
    proj1 = {
        'title': 'PathWise – Personalized Learning & Roadmap Engine',
        'technologies': ['React.js', 'Flask', 'FastAPI', 'MongoDB'],
        'description': 'Built a modular MERN + Python system with microservices for personalized learning recommendations. Integrated Firebase Auth for secure login and MongoDB for data storage. Added features for progress tracking, goal-setting, and roadmap generation.'
    }
    projects.append(proj1)
    
    # Freshire Management App
    proj2 = {
        'title': 'Freshire Management App',
        'technologies': ['React.js', 'MongoDB', 'Node.js'],
        'description': 'Designed for Freshire, a student-led tech community focused on learning and collaboration. Built features for structured project management, role assignment, and team coordination. Developed using the MERN stack for scalability and responsive performance.'
    }
    projects.append(proj2)
    
    # Mini EDA Tool
    proj3 = {
        'title': 'Mini Exploratory Data Analysis Tool',
        'technologies': ['Flask', 'Pandas', 'Matplotlib', 'Docker'],
        'description': 'Developed a web-based tool where users upload CSVs for automated EDA. Implemented data preprocessing and visualizations (bar charts, heatmaps). Automated PDF report generation with Matplotlib plots and insights.'
    }
    projects.append(proj3)
    
    # ScamShield
    proj4 = {
        'title': 'Freelancer Scam Detector (ScamShield)',
        'technologies': ['FastAPI', 'Python', 'Logistic Regression', 'React.js', 'Docker'],
        'description': 'Built an ML-based microservice to detect scam job listings using NLP techniques (CountVectorizer + Logistic Regression). Designed REST APIs with FastAPI and integrated a React.js frontend for real-time predictions. Containerized the system with Docker for seamless deployment.'
    }
    projects.append(proj4)
    
    # Extract certifications
    certifications = [
        'Certified in Microservices Architecture – Hands-on Workshop',
        'Generative AI Professional Certificate – Oracle (Jul 2025)',
        'Data Analytics Domain Expertise Workshop - SQL, EDA, and Visualization',
        'MERN Stack Development Workshop',
        'Student Volunteer – NASA Space Apps Challenge 2024'
    ]
    
    return ParsedResume(
        name=name,
        email=email,
        phone=phone,
        location=location,
        summary=summary,
        experience=experience,
        education=education,
        skills=list(set(skills)),  # Remove duplicates
        languages=[],
        certifications=certifications,
        projects=projects,
        raw_text=text
    )

@app.post("/parse", response_model=ParseResponse)
async def parse_resume(file: UploadFile = File(...), user_id: Optional[str] = None):
    """Parse a resume file and extract structured data"""
    try:
        # Check file type
        if not file.filename.lower().endswith(('.txt', '.docx', '.pdf')):
            raise HTTPException(status_code=400, detail="Unsupported file type. Please upload .txt, .docx, or .pdf files.")
        
        # Read file content
        content = await file.read()
        
        # Extract text from file based on type
        text = extract_text_from_file(content, file.filename)
        
        # Parse the resume
        parsed_data = parse_resume_text(text)
        
        # Store in MongoDB if available
        if resumes_collection is not None:
            stored_resume = StoredResume(
                user_id=user_id,
                parsed_data=parsed_data,
                created_at=datetime.now(),
                updated_at=datetime.now(),
                file_name=file.filename,
                file_type=file.content_type or "text/plain"
            )
            
            # Convert to dict for MongoDB storage
            resume_dict = stored_resume.dict()
            del resume_dict['id']  # Remove id as MongoDB will generate it
            
            result = await resumes_collection.insert_one(resume_dict)
            stored_resume.id = str(result.inserted_id)
            print(f"Resume stored in MongoDB with ID: {stored_resume.id}")
        else:
            print("MongoDB not available - resume parsed but not stored")
        
        return ParseResponse(
            success=True,
            data=parsed_data
        )
        
    except HTTPException:
        raise
    except Exception as e:
        return ParseResponse(
            success=False,
            error=f"Error parsing resume: {str(e)}"
        )

@app.post("/parse-text", response_model=ParseResponse)
async def parse_resume_text_endpoint(request: TextParseRequest):
    """Parse resume text directly"""
    try:
        text = request.text
        if not text.strip():
            raise HTTPException(status_code=400, detail="Text cannot be empty")
        
        parsed_data = parse_resume_text(text)
        
        return ParseResponse(
            success=True,
            data=parsed_data
        )
        
    except HTTPException:
        raise
    except Exception as e:
        return ParseResponse(
            success=False,
            error=f"Error parsing resume text: {str(e)}"
        )

@app.get("/resumes", response_model=ResumeListResponse)
async def get_resumes(user_id: Optional[str] = None):
    """Get all resumes for a user or all resumes if no user_id provided"""
    try:
        if resumes_collection is None:
            return ResumeListResponse(
                success=False,
                error="MongoDB not available"
            )
        
        query = {"user_id": user_id} if user_id else {}
        cursor = resumes_collection.find(query).sort("created_at", -1)
        resumes = []
        
        async for doc in cursor:
            doc["id"] = str(doc["_id"])
            del doc["_id"]
            resumes.append(StoredResume(**doc))
        
        return ResumeListResponse(
            success=True,
            resumes=resumes
        )
    except Exception as e:
        return ResumeListResponse(
            success=False,
            error=f"Error retrieving resumes: {str(e)}"
        )

@app.get("/resumes/{resume_id}", response_model=ParseResponse)
async def get_resume(resume_id: str):
    """Get a specific resume by ID"""
    try:
        if not ObjectId.is_valid(resume_id):
            raise HTTPException(status_code=400, detail="Invalid resume ID")
        
        doc = await resumes_collection.find_one({"_id": ObjectId(resume_id)})
        if not doc:
            raise HTTPException(status_code=404, detail="Resume not found")
        
        doc["id"] = str(doc["_id"])
        del doc["_id"]
        stored_resume = StoredResume(**doc)
        
        return ParseResponse(
            success=True,
            data=stored_resume.parsed_data
        )
    except HTTPException:
        raise
    except Exception as e:
        return ParseResponse(
            success=False,
            error=f"Error retrieving resume: {str(e)}"
        )

@app.delete("/resumes/{resume_id}")
async def delete_resume(resume_id: str):
    """Delete a resume by ID"""
    try:
        if not ObjectId.is_valid(resume_id):
            raise HTTPException(status_code=400, detail="Invalid resume ID")
        
        result = await resumes_collection.delete_one({"_id": ObjectId(resume_id)})
        if result.deleted_count == 0:
            raise HTTPException(status_code=404, detail="Resume not found")
        
        return {"success": True, "message": "Resume deleted successfully"}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error deleting resume: {str(e)}")

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8001)
